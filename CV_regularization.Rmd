---
title: "CV_regularization"
author: "Hong, Yoon-Ho"
date: "2022-09-04"
output: 
  html_document:
    toc: yes
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

모델의 성능을  평가할 때에는 항상 검증셋(test dataset)을 이용해야 합니다. 학습셋(training set)에서는 항상 더 유연한 모델이 더 정확한 예측 결과를 보여줍니다. 그러나, 훈련셋에서 더 정확한 결과가 검정셋에서의 정확성을 보장해주지는 못합니다. 오히려, 유연한 모델일 수록 학습셋의 noise까지 학습하게 됩니다.즉, 과적합(overfit)하게 되어 분산(variance)이 커지게 됩니다. 우리는 편향-분산 절충(bias-variance trade-off)를 고려하여 적절한 수준의 유연성을 갖는 모델을 선택(model selection)해야 합니다.   

검증셋을 이용하여 모델의 성능을 평가하는 방법에는 아래의 세가지가 있습니다.   

1. Validation set approach  
2. LOOCV (Leave-one-out cross-validation)
3. k-fold CV 

auto 데이터셋에서 연비(mpg)를 예측하는 회귀 모델을 만들고, 모델의 성능을 평가해봅시다.  

```{r, message=F}
library(dplyr)
```

```{r}
auto = read.csv("data/auto.csv")
auto = select(auto, -name)
summary(auto)
```

```{r}
set.seed(1)
train_index = sample(nrow(auto), round(nrow(auto)*2/3))
train_data = auto[train_index, ]
test_data = auto[-train_index, ]

lm.fit = lm(mpg ~ ., data = train_data)

pred = predict(lm.fit, test_data)

obs = test_data$mpg

mse = mean((pred - obs)^2)

sqrt(mse)
```

## 검정셋 접근법    

validation set approach 를 10번 반복해서 rmse의 추정치와 분산을 구해봅시다.       
```{r}
rmse_vs = c()

for (i in 1:10){
  train_index = sample(nrow(auto), round(nrow(auto)*2/3), replace = F)
  train_data = auto[train_index, ]
  test_data = auto[-train_index, ]
  lm.fit = lm(mpg ~ ., data = train_data)
  pred = predict(lm.fit, test_data)
  obs = test_data$mpg
  mse = mean((pred - obs)^2)
  rmse_vs[i] = sqrt(mse)
}

mean(rmse_vs); var(rmse_vs)
```

## LOOCV   

LOOCV를 이용하여 평균제곱오차의 추정치와 분산을 구해보자.   
```{r}
rmse_loocv = c()
for (i in 1:nrow(auto)){
  lm.fit = lm(mpg ~ ., data = auto[-i,])
  pred = predict(lm.fit, auto[i,])
  obs = auto$mpg[i]
  mse = (pred - obs)^2
  rmse_loocv[i] = sqrt(mse)
}
mean(rmse_loocv); var(rmse_loocv)
```

## 교차검증법   

K-fold CV  
**rule of thumb: k=5 or 10 (bias-variance trade-off)**

```{r}
library(caret)
folds = createFolds(1:nrow(auto), k=10)
rmse_kcv = c()

for (i in 1:10){
  lm.fit = lm(mpg ~ ., data = auto[-folds[[i]],])
  pred = predict(lm.fit, auto[folds[[i]],])
  obs = auto[folds[[i]],]$mpg
  mse = mean((pred - obs)^2)
  rmse_kcv[i] = sqrt(mse)
}

mean(rmse_kcv); var(rmse_kcv)
```

위 세가지 검증 기법을 비교해봅시다.    

Validation set approach는 training set이 작아서 bias가 큰 경향이 있고, training vs. test dataset 분할의 임의성으로 인해 예측치의 변동성이 보통 크게 나타납니다. 즉, bias와 variance가 모두 큰 경향이 있습니다.      

LOOCV은 training dataset의 크기가 커서 bias를 줄일 수 있고, k-fold CV의 bias는 validation set approach와 LOOCV의 사이에 해당합니다.     

분산은 LOOCV 와 k-fold CV 중 어느 것이 더 크게 나타날까요? 예상과 달리 대부분 k-fold CV이 LOOCV 보다 variance가 더 작은 경향이 있습니다. LOOCV는 training dataset이 서로 매우 유사하므로 모델간의 상관 관계가 매우 높습니다. 모델 간의 상관 관계가 높을 수록 variance가 더 크게 나타납니다.    


## 정규화(Regulariziation)      

> 차원의 저주(Curse of dimensionality)    

in circumstances of p >> n   
using all the features to predict response variable   
will face two drawbacks in terms of ...   
- interpretability  
- increase of variance (overfitting)   
</br>

so, we do   
feature selection   
- subset selection   
- shrinkage (regularization)   
- dimension reduction   

### 부분집합 선택(Subset selection)    

최상의 부분집합 선택   
가능한 모든 경우의 수: $2^p$    


단계적 선택(stepwise selection)       
- 전진(forward)      
- 후진(backward) 
- 하이브리드/혼합(mixed)      

전진과 후진에서 새로운 변수를 모델에 추가(전진)하거나 제거(후진)하는 기준으로는 RSS (residual sum of square) 혹은 $R^2$를 이용합니다.    

최적의 모델 선택   
: 총 p개의 모델 중에서(각 모델은 1부터 p개의 변수를 갖는) 최적의 모델은 교차 검증 기법을 이용해 검정오차를 직접 추정하거나, 간접적으로 추정합니다 (간접 추정에는 Cp, AIC, BIC, adjusted $R^2$를 이용)   

혼합 선택법은 변수들이 모델에 순차적으로 추가된다는 점에서는 전진 기법과 비슷하나, 새로운 변수가 추가됨에 따라 기존의 변수들에 대한 p-value 가 커질 수 있는데, 이것이 어떤 임계치보다 커지면 그 변수를 모델에서 제외합니다.    


## 수축(Shrinkage)    

[Ridge regression](https://www.youtube.com/watch?v=Q81RR3yKn30)    
[Lasso regression](https://www.youtube.com/watch?v=NGf0voTMlcs)     

아래 식을 최소로 하는 $\beta$ 를 추정 (note: shrinkage penalty)     

능형회귀(ridge regresssion)    
$$RSS + \lambda \sum_{j=1}^{p} \beta^2_j$$  
$\lambda$: tuning parameter   

라소회귀(lasso regression)   
$$RSS + \lambda \sum_{j=1}^{p} |\beta_j|$$   


```{r echo=TRUE}
library(glmnet)
``` 


```{r include=FALSE}
library(ISLR)
?Hitters
str(Hitters)
summary(Hitters)
Hitters = na.omit(Hitters)
```


```{r}
x = model.matrix(Salary~., Hitters)[,-1] # create a design matrix, convert factors to a set of dummy variables, remove intercept term   
y = Hitters$Salary 
```

lambda 값의 범위를 설정합니다.     
```{r}
grid = 10^seq(10, -2, length = 100) # lambda를 10^10 에서 10^-2 까지 값을 갖도록 설정 
```


```{r}
ridge.mod = glmnet(x,y,alpha = 0, lambda = grid, standardize = T) # alpha = 0 for ridge regression, alpha = 1 for lasso regression, standardize = TRUE 
```
ridge/lasso regression 에서는 변수들이 모두 동일한 scale을 값도록 표준화해주어야 합니다(모든 변수의 표준편차가 1이 되도록 각 변수의 표준편차로 나누어줌).     

```{r}
plot(ridge.mod)
```

L1 norm 
$$\sum_{j=1}^{p}|\beta_j|$$  


L2 norm   
$$\sqrt{\sum_{j=1}^{p}\beta^2_j}$$    
능형회귀 계수들은 각 lambda 값별로 coef()로 액세스할 수 있는 행렬에 저장됩니다.   
```{r eval = F}
dim(coef(ridge.mod))
```

lambda 값이 클 때 L2 norm은 lambda 값이 작을 때의 L2 norm에 비해 작습니다.   

100개의 lambda 값 중 50번째의 L2 norm 을 계산해봅시다.    
```{r}
ridge.mod$lambda[50]
```

```{r}
coef(ridge.mod)[-1,50] # at 50th lambda, coefficients
```

```{r}
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # L2 norm at 50th lambda 
```

검정셋 기법을 이용해 검정오차를 추정해봅시다.   
```{r}
set.seed(1)
train = sample(nrow(x), nrow(x)/2)
y.test = y[-train]
```

임의로 lamda 값이 4 일때, ridge regression model의 계수 추정치를 구해봅시다.        
```{r}
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid)
ridge.pred = predict(ridge.mod, s=4, newx = x[-train,])
mse = mean((ridge.pred-y.test)^2)
sqrt(mse)
```

lamda 값을 매우 큰 모델을 적합하여(10^10) 계수 추정치를 구해봅시다.        
```{r}
ridge.pred = predict(ridge.mod, s=10^10, newx = x[-train,])
mse = mean((ridge.pred-y.test)^2)
sqrt(mse)
```

penalty term을 없앨 때(즉, 최소제곱 적합)의 결과와 비교해봅시다.
```{r}
ridge.pred = predict(ridge.mod, s=0, newx = x[-train,])
mse = mean((ridge.pred-y.test)^2)
sqrt(mse)
```   

임의로 lambda 값을 선택하는 대신에 교차검증을 사용하여 결정하도록 합시다. 즉, 교차검증을 통해 검정오차 추정치가 가장 낮은 lambda 값을 구합니다.   

```{r}
set.seed(1)
cv.ridge = cv.glmnet(x[train,], y[train], alpha = 0, nfolds = 10)
plot(cv.ridge)
```

```{r}
bestlambda.ridge = cv.ridge$lambda.min
bestlambda.ridge
```

cv 검정오차 추정치가 가장 낮은 lambda 값에 연관된 검정 MSE를 구해봅시다.    
```{r}
ridge.pred = predict(ridge.mod, s=bestlambda.ridge, newx = x[-train,])
mse = mean((ridge.pred - y.test)^2)
sqrt(mse)
```

마지막으로 교차검증에 의해 선택된 lambda 값을 사용하여 전체 데이터셋에 ridge regression 모델을 적합하고 coefficients 추정치를 구해봅시다.   

```{r}
ridge.out = glmnet(x,y,alpha = 0)
predict(ridge.out, type = "coefficients", s=bestlambda.ridge)
```

lasso  
```{r}
lasso.mod = glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
cv.lasso = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.lasso)
```

```{r}
bestlambda.lasso = cv.lasso$lambda.min
lasso.pred = predict(lasso.mod, s=bestlambda.lasso, newx = x[-train,])
mse = mean((lasso.pred - y.test)^2)
sqrt(mse)
```

```{r}
lasso.out = glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef = predict(lasso.out, type = "coefficients",s=bestlambda.lasso)
lasso.coef
```

linear regression 
```{r}
lm.fit = lm(Salary~., data = Hitters, subset = train)
lm.pred = predict(lm.fit, newdata = Hitters[-train,])
mse = mean((lm.pred - y.test)^2)
sqrt(mse)
```




