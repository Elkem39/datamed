---
title: "Unsupervised learning: PCA, clustering"
author: "Hong, Yoon-Ho"
date: "2022-09-18"
output: 
  html_document: 
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

비지도기계학습이란?     

- 주성분분석(Principal component analysis, PCA)  
- 군집분석(Clustering)   
    - K-means clustering 
    - 계층적 군집분석(Hierchical clustering) 

PCA  
: dimension reduction for visualization and/or preprocessing of  high-dimensional data   
   
   
Clustering
: subgrouping of predictors or observations based on similarity (distance)   


## 주성분분석(PCA) 

첫번째 주성분(the first principal component)    
: 변수들의 정규화된 (normalized) 선형결합 (가장 큰 분산을 가지는, 즉 관측치가 가장 많이 변화되는 변수공간의 방향)  

$$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + ... + \phi_{p1}X_p$$ 

$$\sum_{j=1}^p \phi^2_{j1} = 1$$

두번째 주성분(the second principal component)      
: 첫번째 주성분과 상관되지 않은 (uncorrelated, orthogonal) X1,X2,,,Xp 의 모든 선형결합 중에서 분산을 최대로 하는 선형결합   
  
  
변수 스케일링(scaling)      
: 스케일링되지 않은 변수에 PCA 를 수행하면 단순히 분산이 큰 변수에 의해 주성분벡터가 결정됨   

로딩 벡터와 주성분 점수    
- 로딩 벡터(loading vector, length=p)

$$\phi_1 = (\phi_{11}, \phi_{21},,,\phi_{p1})^T$$
- 주성분 점수(principal component scores, length=n) 
각 관측치의 값을 주성분(principal component)으로 변환한 것으로 해당 주성분(principal component)별로 있음      

예를 들어, 첫번째 주성분(principal component)에 대한 각 관측치의 주성분 점수(principal component scores) 값은 아래와 같이 표현함    

(z11,z21,,,zn1)   
  
## 실습(PCA)   

```{r}
states = row.names(USArrests) 
states # 50 states in USA 
```

```{r}
names(USArrests)
```

```{r}
summary(USArrests)
```


```{r}
apply(USArrests, 2, var) # column-wise 
```


```{r}
apply(USArrests, 2, mean) # column-wise 
```


```{r}
pr.out = prcomp(USArrests, scale = TRUE) 
names(pr.out)
```

```{r}
pr.out$rotation # loading vectors 
```


```{r}
head(pr.out$x) # pc scores
```



```{r}
biplot(pr.out)
```

```{r}
pr.out$rotation = -pr.out$rotation # loading vectors 
pr.out$x = -pr.out$x # pc scores
biplot(pr.out) 
```

```{r}
summary(pr.out)
```

```{r}
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var)
plot(pve, xlab = "Principal component", ylab = "Proportion of Variance Explained", type = "b") # scree plot 
```

Another example for PCA 
```{r}
library(ISLR) # NCI60 data in this package 
```

```{r}
class(NCI60) # NCI60: 64개 암세포주에 대한 6830개의 유전자 발현 관측치 
nci.labs = NCI60$labs
table(nci.labs) # 14개 유형의 암세포주 
```

```{r}
nci.data = NCI60$data
dim(nci.data)
```

```{r}
nci_pc_out = prcomp(nci.data, scale = TRUE) 
```

```{r}
library(ggplot2)
pc12scores = nci_pc_out$x[,c("PC1", "PC2")]
df = data.frame(cell_lines = nci.labs, pc12scores)
ggplot(data = df, aes(x=PC1, y=PC2, col = cell_lines)) + geom_point(size = 3)
```

```{r}
summary(pr.out)
```


## Hierachical clustering 

예를 들어 두 개의 관측치가 서로 유사하다는 것을 어떻게 결정할지 생각해봅시다. 

유사성(similarity)의 척도가 필요한데, 이를 위해 우리는 거리(distance)를 계산합니다.      

유사성의 척도(거리)     
 
*Euclidean distance*    
$$d_{euc}(x,y) = \sqrt{\sum_{i=1}^n(x_i - y_i)^2}$$ 

*Manhattan distance*   
$$d_{man}(x,y) = \sum_{i=1}^n |{(x_i - y_i)|}$$  

*pearson correlation distance*      
$$d_{cor}(x, y) = 1 - \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum\limits_{i=1}^n(x_i - \bar{x})^2 \sum\limits_{i=1}^n(y_i -\bar{y})^2}}$$   
* correlation distance 는 관측치들의 크기보다는 그들의 모양(프로파일)에 중점을 둔다.   

스케일링: 관측치간의 유사성을 계산하기 전에 각 변수에 주어진 중요도를 동일하게 만들기 위해서 표준편차가 1이 되도록 한다. 이것은 k-mean clustering 에서도 동일하게 적용된다.   

> 알고리즘(aggolomerative hierachical clustering의 경우)   
1. n개 관측치에서 모든 쌍별로 비유사성의 척도(예를 들어, 유클리드 거리)를 계산한다. 각 관측치 자체를 하나의 클러스터로 취급한다.   
2. 모든 클러스터들간의 쌍별 거리를 계산하여, 가장 거리가 작은(즉, 가장 유사한) 클러스터 쌍을 융합한다. 
3. 위에서 융합된 새로운 클러스터를 포함하여, 다시 2번 과정을 반복한다.   
만약, 클러스터가 두 개 이상의 관측치들을 포함하는 경우, 클러스터 사이의 비유사성을 어떻게 정의할 것인가?  

*linkage* 방법들        
- complete linkage   
클러스터 간 거리의 최대치          
- average linkage   
클러스터 간 거리의 평균치   
- single linkage  
클러스터 간 거리의 최소치 
- centroid linkage   
클러스터 간 무게중심사이의 거리   

complete, average linkage 가 좀 더 균형잡힌 클러스터들을 제공하는 경향이 있다.    

## 실습(Hierachical clustering)   

```{r}
nci_scaled = scale(nci.data) # center and scale are TRUE in default  
```


```{r}
dist = dist(nci_scaled)
hclust.complete = hclust(dist, method = "complete")
```


```{r}
plot(hclust.complete, labels = nci.labs, main = "complete linkage")
```

```{r}
hc.clusters = cutree(hclust.complete, 5)
hc.clusters
```

```{r}
table(hc.clusters, nci.labs)
```


## K-means clustering 

n개 관측치들을 K개 클러스터로 분할하는 방법: Kn

> 알고리즘 
1. 각 관측치에 1에서 K까지의 숫자를 랜덤하게 할당한다. 이것은 관측체에 대한 초기 클러스터 할당으로 작용한다. 
2. 클러스터 할당이 변하지 않을 때까지 다음을 반복한다. 
  + 2-1. 클러스터 각각에 대해 클러스터 무게중심을 계산한다.     
  + 2-2. 각 관측치와 각 클러스터의 무게중심 간의 거리를 계산한다.  
  + 2-3. 무게 중심이 가장 가까운 클러스터로 각 관측치를 새롭게 할당한다. 
*주의   
K 를 선택하는 것은 전혀 단순한 문제가 아니다.    

## 실습(K-means clustering)   

```{r}
set.seed(2)
km.out = kmeans(nci_scaled, centers = 5, nstart = 20)
```


```{r}
km.clusters = km.out$cluster
km.clusters
```

```{r}
table(km.clusters, hc.clusters)
```


## Heatmap 


```{r}
df_scaled = scale(USArrests)
summary(df_scaled)
```

```{r}
df_dist = dist(df_scaled) # row-wise distance matrix 
hc_out = hclust(df_dist, method = "complete")
plot(hc_out)
```

```{r}
hc_clusters = cutree(hc_out, 4)
hc_clusters
```


load package
```{r}
library(pheatmap)
```

```{r}
mat = as.matrix(t(df_scaled))
apply(mat, 1, sd) # 1 for row-wise 
```


```{r}
pheatmap(mat, 
         cluster_rows = T,
         # cellheight = 20,
         annotation_col = data.frame(hc_clusters))
```


